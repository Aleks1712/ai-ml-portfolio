{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2ec9aff7",
      "metadata": {},
      "source": [
        "# Intro to LLMs (LLM-delen) — ryddige notater + kode\n",
        "\n",
        "Dette er en “ryggmargs”-intro til **Large Language Models** (LLMs):\n",
        "\n",
        "- hva de er\n",
        "- hva de kan gjøre\n",
        "- hvordan de funker (Transformer)\n",
        "- hvordan du bruker dem i egne prosjekter (OpenAI / LangChain / Hugging Face)\n",
        "- hvorfor dette er relevant for bachelor (sikkerhet, reproducibility)\n",
        "\n",
        "> GitHub-tips: clear output før du pusher.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09147324",
      "metadata": {},
      "source": [
        "## 1) Hva er en LLM?\n",
        "\n",
        "En **Large Language Model** er en språkmodell (ofte Transformer-basert) som er trent på enorme mengder tekst for å lære mønstre i språk.\n",
        "\n",
        "### Kortversjon\n",
        "\n",
        "- **Input**: tekst → tokeniseres\n",
        "- **Modell**: lærer statistiske/semantiske mønstre og kontekst\n",
        "- **Output**: genererer tekst (decoder-only, f.eks. GPT) eller lager representasjoner/klassifisering (encoder, f.eks. BERT)\n",
        "\n",
        "### Typiske egenskaper\n",
        "\n",
        "- **general purpose**: kan brukes til mange oppgaver\n",
        "- **pre-trained** på store datasett, og kan så **fine-tunes** på spesifikke oppgaver\n",
        "- kan fungere i **zero-shot** og **few-shot** settinger (prompting)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "795cd968",
      "metadata": {},
      "source": [
        "## 2) Hva kan LLMs brukes til?\n",
        "\n",
        "Eksempler (du nevnte mange av disse):\n",
        "\n",
        "- **tekstgenerering**: essays, e-post, poesi\n",
        "- **oversettelse**: språk → språk\n",
        "- **spørsmål/svar**: FAQ, dokument-QA\n",
        "- **chatbots**: kundeservice, intern assistent\n",
        "- **koding**: forslag, debugging, refactoring\n",
        "- **analyse**: oppsummering, keyword extraction, klassifisering\n",
        "- **domene**: finance, jus, helse (men krever ekstra kontroll/validering)\n",
        "\n",
        "> Viktig i “ekte systemer”: kombiner ofte LLM med verktøy (retrieval, databaser, regler) for sporbarhet og sikkerhet.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bfda2dc",
      "metadata": {},
      "source": [
        "## 3) Hvor store er LLMs (parametre) — og hvorfor betyr det noe?\n",
        "\n",
        "- **Parametre** = “vekter” i nettverket (millioner → milliarder)\n",
        "- Grovt: flere parametre + mer data kan gi bedre språkforståelse\n",
        "\n",
        "Men:\n",
        "\n",
        "- det er ikke *bare* størrelse (data-kvalitet, arkitektur, trening, alignment, osv.)\n",
        "- større modeller kan være dyrere, tregere, og mer krevende å sikre\n",
        "\n",
        "### LLM-trening i to steg\n",
        "\n",
        "- **Pre-training**: lærer generelle språk-mønstre på store korpus (bøker, web, osv.)\n",
        "- **Fine-tuning / instruction tuning**: gjør modellen mer spesifikk (oppgave, stil, sikkerhet)\n",
        "\n",
        "### Prompting\n",
        "\n",
        "- **zero-shot**: bare instruksjon\n",
        "- **few-shot**: gir 1–N eksempler i prompten\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c62ff379",
      "metadata": {},
      "source": [
        "## 4) Hvordan fungerer de? (Transformer – \"Attention is All You Need\")\n",
        "\n",
        "### A) Tokenisering → embeddings\n",
        "\n",
        "Tekst blir gjort om til **tokens** (subwords), og hvert token blir en vektor (**embedding**).\n",
        "\n",
        "### B) Self-attention\n",
        "\n",
        "Modellen regner ut hvilke tokens som er viktigst for hverandre (kontekst).\n",
        "\n",
        "- **Multi-head attention** lar modellen fange flere typer relasjoner samtidig.\n",
        "\n",
        "### C) Arkitekturtyper\n",
        "\n",
        "- **Encoder-only** (BERT/RoBERTa/DistilBERT): gode på forståelse/klassifisering/QA (extractive)\n",
        "- **Decoder-only** (GPT): gode på generering/chat\n",
        "- **Encoder–decoder** (T5/BART): ofte brukt til oversettelse/summarization\n",
        "\n",
        "Huske-regel:\n",
        "\n",
        "- Encoder = “les/forstå”\n",
        "- Decoder = “skriv/fortsett”\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab454533",
      "metadata": {},
      "source": [
        "## 5) GPT, BERT, XLNet, LLaMA — hva er forskjellen?\n",
        "\n",
        "### GPT (decoder-only)\n",
        "\n",
        "- genererer tekst token-for-token\n",
        "- brukes ofte til chat, generering, “reasoning” og tool-use\n",
        "\n",
        "### BERT (encoder-only)\n",
        "\n",
        "- trener på Masked Language Modeling (MLM)\n",
        "- veldig sterk på klassifisering, NER, semantisk søk, extractive QA\n",
        "\n",
        "### XLNet\n",
        "\n",
        "- alternativ pretraining som kan gi gode resultater på noen tasks\n",
        "- ofte brukt i klassifisering (historisk)\n",
        "\n",
        "### LLaMA (familie av åpne modeller)\n",
        "\n",
        "- open-weight modeller (varierer per versjon/lisens)\n",
        "- ofte brukt lokalt / i egen infrastruktur\n",
        "\n",
        "> Bachelor-relevans: modelltype påvirker angrepsflate (tokenisering, special tokens, prompt-format, osv.).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5193eff",
      "metadata": {},
      "source": [
        "## 6) Hugging Face: lagring og lasting av modell (Bachelor-notater)\n",
        "\n",
        "### Hvorfor lagre både modell og tokenizer?\n",
        "\n",
        "I LLM-sikkerhet/backdoor/defense er **reproduserbarhet** alt.\n",
        "\n",
        "> En modell uten riktig tokenizer er i praksis en annen modell.\n",
        "\n",
        "### Hva lagres?\n",
        "\n",
        "- Tokenizer: `tokenizer.json`, vocab/merges/sentencepiece, special tokens, mapping\n",
        "- Modell: `pytorch_model.bin` + `config.json` (+ ev. `safetensors`)\n",
        "\n",
        "### Typisk workflow\n",
        "\n",
        "1. Evaluer baseline (CACC/ASR)\n",
        "2. Lagre checkpoint\n",
        "3. Kjør defense\n",
        "4. Lagre ny checkpoint\n",
        "5. Sammenlign før/etter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8dd7dd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hugging Face: save_pretrained / from_pretrained (eksempel)\n",
        "# (Krever: transformers)\n",
        "\n",
        "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "#\n",
        "# model_name = \"distilbert-base-uncased\"\n",
        "# save_dir = \"models/distilbert-base-uncased\"\n",
        "#\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "#\n",
        "# tokenizer.save_pretrained(save_dir)\n",
        "# model.save_pretrained(save_dir)\n",
        "#\n",
        "# # reload\n",
        "# tok2 = AutoTokenizer.from_pretrained(save_dir)\n",
        "# model2 = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
        "#\n",
        "# print(\"Saved and reloaded OK\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8602ab02",
      "metadata": {},
      "source": [
        "## 7) Special tokens (viktig for både NLP og sikkerhet)\n",
        "\n",
        "Special tokens gir modellen struktur (start/slutt, separator, padding, osv.).\n",
        "\n",
        "- BERT: `[CLS]`, `[SEP]`, `[PAD]`, `[MASK]`\n",
        "- RoBERTa: `<s>`, `</s>` (varierer)\n",
        "- GPT: ofte `<bos>`, `<eos>` (varierer)\n",
        "\n",
        "**Hvorfor det betyr noe i bachelor:**\n",
        "\n",
        "- triggere kan være token-spesifikke\n",
        "- padding/truncation kan skjule eller forsterke triggere\n",
        "- endrer du tokenizer, endrer du angrepsflaten\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d2e5830",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se special tokens i en tokenizer (eksempel)\n",
        "# (Krever: transformers)\n",
        "\n",
        "# from transformers import AutoTokenizer\n",
        "# tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# print(tok.special_tokens_map)\n",
        "# print(\"PAD id:\", tok.pad_token_id)\n",
        "# print(\"CLS id:\", tok.cls_token_id)\n",
        "# print(\"SEP id:\", tok.sep_token_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5a5a309",
      "metadata": {},
      "source": [
        "## 8) Fra LLM til ditt prosjekt: en praktisk mal\n",
        "\n",
        "### A) Ren LLM (uten retrieval)\n",
        "\n",
        "Bra for:\n",
        "\n",
        "- ideer, tekst, forklaringer\n",
        "- små kodehjelp\n",
        "\n",
        "Ulempe:\n",
        "\n",
        "- kan hallusinere\n",
        "- ikke oppdatert på dine dokumenter\n",
        "\n",
        "### B) RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "Når du vil svare med *dine* dokumenter:\n",
        "\n",
        "1. last dokumenter\n",
        "2. chunk/splitt\n",
        "3. embeddings\n",
        "4. lagre i vector store\n",
        "5. retrieval + LLM-svar\n",
        "\n",
        "### C) Logging + eval\n",
        "\n",
        "- logg prompts, modell, temperatur, max tokens\n",
        "- evaluer output (kvalitet, sikkerhet, ASR/CACC hvis backdoor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03d53a23",
      "metadata": {},
      "source": [
        "## 9) OpenAI API (eksempel) — *bruk env vars*\n",
        "\n",
        "Du hadde et eksempel med `openai==0.28`. Det funker, men moderne SDK-er har litt annen stil.\n",
        "\n",
        "**Best practice (GitHub):**\n",
        "\n",
        "- legg `OPENAI_API_KEY` i `.env`\n",
        "- last med `%dotenv` eller `python-dotenv`\n",
        "- aldri commit `.env`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f243ac2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# OpenAI (legacy-style, omtrent som openai==0.28)\n",
        "# NB: Kjør bare hvis du har openai-pakken installert og OPENAI_API_KEY satt.\n",
        "\n",
        "# import os\n",
        "# import openai\n",
        "#\n",
        "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "#\n",
        "# def generate_text(prompt: str, max_tokens: int = 50, temperature: float = 0.7) -> str:\n",
        "#     resp = openai.Completion.create(\n",
        "#         engine=\"davinci-002\",\n",
        "#         prompt=prompt,\n",
        "#         max_tokens=max_tokens,\n",
        "#         temperature=temperature,\n",
        "#     )\n",
        "#     return resp.choices[0].text.strip()\n",
        "#\n",
        "# print(generate_text(\"Once upon a time\", max_tokens=30, temperature=0.0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b48e4b11",
      "metadata": {},
      "source": [
        "## 10) Mini-del: QA (BERT) + klassifisering (XLNet) — rapportvennlig\n",
        "\n",
        "Dette er to veldig “klassiske” NLP-demoer som også er fine å beskrive i bachelor:\n",
        "\n",
        "- **Extractive QA**: modellen velger start/slutt-posisjon i kontekst\n",
        "- **Klassifisering**: modellen predikerer label (følelse, spam, topic, osv.)\n",
        "\n",
        "Hvis du vil, kan jeg lage disse som egne notebooks i `nlp/` senere.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f56f9944",
      "metadata": {},
      "source": [
        "## 11) LLM-sikkerhet: angrep vs defense (rammeverk)\n",
        "\n",
        "### Attack surface (hvor angrep kan skje)\n",
        "\n",
        "- **data-nivå**: poisoning/backdoor i treningsdata\n",
        "- **modell-nivå**: kompromitterte checkpoints (supply chain)\n",
        "- **prompt/kontekst**: prompt injection, RAG-dokumenter med instruksjoner\n",
        "\n",
        "### Defense (typisk pipeline)\n",
        "\n",
        "- data filtering/anomali\n",
        "- post-training defenses (pruning/unlearning/merge)\n",
        "- inference defenses (sanitization, prompt firewall)\n",
        "\n",
        "### Evaluering\n",
        "\n",
        "- **CACC** (clean accuracy) og **ASR** (attack success rate)\n",
        "- logg alt: modellnavn, seed, hyperparametre, triggere\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f95cfad9",
      "metadata": {},
      "source": [
        "## 12) Kurs-repo / miljø (fra notatene dine)\n",
        "\n",
        "Du limte inn info fra et repo med \"Introduction to LLMs\".\n",
        "\n",
        "### Eksempel på pakker (kurs-miljø)\n",
        "\n",
        "*(Dette er “pinne”-versjoner fra kurset; du kan bruke nyere i eget prosjekt.)*\n",
        "\n",
        "- python 3.11\n",
        "- `transformers`, `torch`, `datasets`, `evaluate`, `accelerate`\n",
        "- `langchain`, `faiss-cpu`, `tiktoken`\n",
        "\n",
        "### Conda eksempel (rettet kommando-format)\n",
        "\n",
        "```bash\n",
        "conda create --name llms_course_env python=3.11\n",
        "conda activate llms_course_env\n",
        "pip install ipykernel jupyterlab notebook\n",
        "python -m ipykernel install --user --name=llms_course_env --display-name \"llms_course_env\"\n",
        "```\n",
        "\n",
        "> Hvis du heller bruker `venv` (som i `03-setting-up-the-env.ipynb`), er det helt fint.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
