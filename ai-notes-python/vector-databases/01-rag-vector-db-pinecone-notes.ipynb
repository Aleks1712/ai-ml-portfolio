{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a0e020ab",
      "metadata": {},
      "source": [
        "# RAG + Vector DB + Pinecone (notater + kode-maler)\n",
        "\n",
        "Dette er en “bachelor-/produksjonsvennlig” samling notater du kan bruke i rapport og implementasjon.\n",
        "\n",
        "**Tema:**\n",
        "\n",
        "- når du trenger RAG + vector DB\n",
        "- chunking/splitting (context window)\n",
        "- embeddings + vector space (cosine/dot/euclid)\n",
        "- retrieval: similarity vs MMR\n",
        "- stuffing (stuff chain)\n",
        "- Pinecone (managed vector DB)\n",
        "- mini-arkitektur (klar til rapport)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b4ad4dd",
      "metadata": {},
      "source": [
        "## 1) Hva prøver du å løse med RAG + Vector DB?\n",
        "\n",
        "Tenk på to typer spørsmål:\n",
        "\n",
        "1. **Eksakte spørsmål (SQL/NoSQL passer)**\n",
        "   - “Finn ordre med `id=123`”\n",
        "   - “Hent kunden med e-post X”\n",
        "\n",
        "2. **Semantiske spørsmål (Vector DB passer)**\n",
        "   - “Hva sier runbooken om *high CPU*?”\n",
        "   - “Hvordan håndterer vi chargeback/dispute?”\n",
        "\n",
        "**RAG** bygger bro:\n",
        "\n",
        "- Vector DB finner relevante tekstbiter (**retrieval**)\n",
        "- LLM bruker dem som kontekst for å svare (**generation**)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3127b83e",
      "metadata": {},
      "source": [
        "## 2) Loading + splitting (chunking)\n",
        "\n",
        "### Hvorfor splitting?\n",
        "\n",
        "LLM-er har et **kontekstvindu** (maks tokens per forespørsel). Du kan ikke sende hele PDF/SLA/runbook i én prompt.\n",
        "\n",
        "Du deler derfor inn i **chunks** som:\n",
        "\n",
        "- er små nok til å passe inn\n",
        "- er store nok til å gi mening\n",
        "- har **overlap** slik at kontekst ikke “knekker”\n",
        "\n",
        "### Chunking styres av\n",
        "\n",
        "- **chunk_size** (f.eks. 1500 tegn)\n",
        "- **chunk_overlap** (f.eks. 500 tegn)\n",
        "- **separators** (f.eks. `\\n\\n`, `\\n`, `. `, ` `)\n",
        "\n",
        "### Splittere (praktisk)\n",
        "\n",
        "- `CharacterTextSplitter`: enkel fast oppdeling\n",
        "- `RecursiveCharacterTextSplitter`: prøver snille grenser først (ofte best default)\n",
        "- `TokenTextSplitter`: splitter på tokens (mer presist mot kontekstvindu)\n",
        "- `MarkdownHeaderTextSplitter`: splitter på `# / ## / ###` (perfekt for notater)\n",
        "\n",
        "### Metadata = gull i bachelor\n",
        "\n",
        "Lag chunks som `Document` med:\n",
        "\n",
        "- `page_content`\n",
        "- `metadata` (tittel, side, filnavn, URL)\n",
        "\n",
        "Da kan du skrive: “Hentet fra Lecture X / side Y”.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fd23086",
      "metadata": {},
      "source": [
        "## 3) Embeddings og vector space (intuition)\n",
        "\n",
        "### Hva er embedding?\n",
        "\n",
        "En **embedding** er en vektor (liste med tall) som representerer “meningen” i en tekstbit.\n",
        "\n",
        "Du kan formulere det slik:\n",
        "\n",
        "> Embeddings projiserer abstrakte konsepter inn i et kontinuerlig numerisk rom der semantisk likhet kan måles med avstand/similaritet.\n",
        "\n",
        "### Likhet/avstand\n",
        "\n",
        "- **Cosine similarity**: vinkelen mellom vektorer (retning ≈ mening)\n",
        "- **Dot product**: retning + magnitude (raskt; hvis vektorer normaliseres ≈ cosine)\n",
        "- **Euclidean distance**: “luftlinje-avstand”\n",
        "- **Manhattan distance**: “rutenett-avstand” (sjeldnere brukt i tekst-embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "976f8e77",
      "metadata": {},
      "source": [
        "## 4) SQL vs NoSQL vs Vector DB (rapport-linje)\n",
        "\n",
        "- **SQL**: eksakt match + relasjoner + transaksjoner\n",
        "- **NoSQL**: fleksible dokumenter/key-value\n",
        "- **Vector DB**: semantisk match (nærmeste nabo i embedding-rom), ofte med metadata-filter\n",
        "\n",
        "Paste-ready setning:\n",
        "\n",
        "> SQL/NoSQL er optimalisert for eksakt matching og strukturerte spørringer, mens vektordatabaser er optimalisert for semantisk matching der likhet beregnes i et embedding-rom.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95902714",
      "metadata": {},
      "source": [
        "## 5) Retrieval: similarity search vs MMR\n",
        "\n",
        "### Similarity search\n",
        "\n",
        "Henter top-k mest like chunks.\n",
        "\n",
        "- Ulempe: du kan få duplikater/nesten-like chunks (støy)\n",
        "\n",
        "### MMR (Max Marginal Relevance)\n",
        "\n",
        "Henter chunks som er:\n",
        "\n",
        "- relevante **og**\n",
        "- mer **diverse**\n",
        "\n",
        "Eksempel (SLA/logistikk):\n",
        "\n",
        "- similarity: 5 nesten identiske “delay”-avsnitt\n",
        "- MMR: delay + escalation + compensation + frister\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc8bc885",
      "metadata": {},
      "source": [
        "## 6) Stuffing (“stuff chain”)\n",
        "\n",
        "**Stuffing** betyr: hentede chunks settes direkte inn i prompt-konteksten.\n",
        "\n",
        "Fordeler:\n",
        "\n",
        "- enklest å implementere\n",
        "- lett å forklare i rapport\n",
        "\n",
        "Ulemper (akademisk viktig):\n",
        "\n",
        "- kan sprenge kontekstvindu\n",
        "- hvis retrieval gir støy, “stuffer” du støy\n",
        "\n",
        "Paste-ready setning:\n",
        "\n",
        "> I denne implementasjonen brukes en “stuffing”-strategi der top-k hentede chunks legges direkte i promptkonteksten. Dette forenkler arkitekturen, men krever stram kontroll på chunk-størrelse, overlap og antall dokumenter for å unngå kontekstovertrekk og irrelevante svar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "750efb99",
      "metadata": {},
      "source": [
        "## 7) Pinecone (hva og hvorfor)\n",
        "\n",
        "Pinecone er en managed vector database som lar deg:\n",
        "\n",
        "- opprette indeks (dimension + metric, ofte cosine)\n",
        "- upsert embeddings + metadata\n",
        "- query med semantisk søk + metadata-filter\n",
        "\n",
        "Brukes når du vil ha:\n",
        "\n",
        "- skalerbar og driftbar vector store\n",
        "- rask ANN-søk\n",
        "- mindre “DIY” enn lokal Chroma\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "481fb6a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kode-mal: RAG med LangChain + Chroma (template)\n",
        "# (Krever pakker: langchain/langchain-openai/langchain-community/chromadb/python-dotenv)\n",
        "\n",
        "# import os\n",
        "# from dotenv import load_dotenv\n",
        "# load_dotenv()\n",
        "# assert os.getenv(\"OPENAI_API_KEY\")\n",
        "#\n",
        "# from langchain_community.document_loaders import TextLoader\n",
        "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "# from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "# from langchain_community.vectorstores import Chroma\n",
        "# from langchain_core.prompts import PromptTemplate\n",
        "# from langchain_core.runnables import RunnablePassthrough\n",
        "# from langchain_core.output_parsers import StrOutputParser\n",
        "#\n",
        "# docs = TextLoader(\"data/my_corpus.txt\", encoding=\"utf-8\").load()\n",
        "# splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=500)\n",
        "# chunks = splitter.split_documents(docs)\n",
        "#\n",
        "# emb = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "# vectordb = Chroma.from_documents(chunks, emb, persist_directory=\"chroma_db\")\n",
        "# vectordb.persist()\n",
        "#\n",
        "# retriever = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5, \"fetch_k\": 20})\n",
        "#\n",
        "# def format_docs(docs):\n",
        "#     out = []\n",
        "#     for d in docs:\n",
        "#         src = d.metadata.get(\"source\", \"unknown\")\n",
        "#         out.append(f\"[{src}] {d.page_content}\")\n",
        "#     return \"\\n\\n\".join(out)\n",
        "#\n",
        "# llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "# prompt = PromptTemplate.from_template(\n",
        "#     \"Du er en presis fagassistent. Svar kun basert på konteksten.\\n\"\n",
        "#     \"Hvis ikke nok kontekst: skriv 'Ikke nok informasjon i kildene'.\\n\\n\"\n",
        "#     \"Spørsmål: {question}\\n\\nKontekst:\\n{context}\\n\"\n",
        "# )\n",
        "#\n",
        "# chain = (\n",
        "#     {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "#     | prompt\n",
        "#     | llm\n",
        "#     | StrOutputParser()\n",
        "# )\n",
        "#\n",
        "# print(chain.invoke(\"Forklar forskjellen på similarity og MMR\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f4e7c28",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kode-mal: Pinecone + LangChain (template)\n",
        "# NB: Dette er en mal basert på notatene dine. Tilpass dimensjon/model.\n",
        "\n",
        "# import os\n",
        "# from dotenv import load_dotenv\n",
        "# load_dotenv()\n",
        "#\n",
        "# from pinecone import Pinecone, ServerlessSpec\n",
        "# from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "# from langchain_community.document_loaders import TextLoader\n",
        "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "# from langchain_core.prompts import ChatPromptTemplate\n",
        "#\n",
        "# pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
        "# index_name = \"my-index\"\n",
        "# dimension = 1536  # må matche embedding-dimensjonen\n",
        "# metric = \"cosine\"\n",
        "#\n",
        "# existing = [x[\"name\"] for x in pc.list_indexes().get(\"indexes\", [])]\n",
        "# if index_name not in existing:\n",
        "#     pc.create_index(\n",
        "#         name=index_name,\n",
        "#         dimension=dimension,\n",
        "#         metric=metric,\n",
        "#         spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "#     )\n",
        "#\n",
        "# index = pc.Index(index_name)\n",
        "#\n",
        "# # docs -> chunks\n",
        "# docs = TextLoader(\"data/corpus.txt\", encoding=\"utf-8\").load()\n",
        "# splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=500)\n",
        "# chunks = splitter.split_documents(docs)\n",
        "# for d in chunks:\n",
        "#     d.page_content = \" \".join(d.page_content.split())\n",
        "#\n",
        "# emb = OpenAIEmbeddings()\n",
        "# texts = [d.page_content for d in chunks]\n",
        "# metas = [d.metadata for d in chunks]\n",
        "# vectors = emb.embed_documents(texts)\n",
        "#\n",
        "# to_upsert = []\n",
        "# for i, (vec, meta, text) in enumerate(zip(vectors, metas, texts)):\n",
        "#     meta = dict(meta)\n",
        "#     meta[\"text\"] = text\n",
        "#     to_upsert.append((f\"doc-{i}\", vec, meta))\n",
        "#\n",
        "# index.upsert(vectors=to_upsert)\n",
        "#\n",
        "# # query + stuffing\n",
        "# q = \"Forklar forskjellen på similarity search og MMR i retrieval\"\n",
        "# qvec = emb.embed_query(q)\n",
        "# res = index.query(vector=qvec, top_k=5, include_metadata=True)\n",
        "# ctx = \"\\n\\n---\\n\\n\".join([m[\"metadata\"].get(\"text\", \"\") for m in res[\"matches\"]])\n",
        "#\n",
        "# llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "# prompt = ChatPromptTemplate.from_messages([\n",
        "#     (\"system\", \"Svar kun basert på konteksten. Hvis ikke nok: 'Ikke nok informasjon i kildene'.\"),\n",
        "#     (\"user\", \"Spørsmål: {q}\\n\\nKontekst:\\n{ctx}\"),\n",
        "# ])\n",
        "#\n",
        "# answer = llm.invoke(prompt.format_messages(q=q, ctx=ctx)).content\n",
        "# print(answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e6a4af0",
      "metadata": {},
      "source": [
        "## 8) Mini-arkitektur (klar til rapport)\n",
        "\n",
        "**Indexing (offline):**\n",
        "\n",
        "Load → Clean → Split → Embed → Store (Vector DB)\n",
        "\n",
        "**Query-time (online):**\n",
        "\n",
        "Embed query → Retrieve (Similarity/MMR) → Stuff context → Prompt → LLM → Parse/Validate → Log/Trace\n",
        "\n",
        "Figurtekst (paste-ready):\n",
        "\n",
        "> Figur X: RAG-pipeline modellert som graf: dokumenter splittes i overlappende chunks, embeddes til vektorer og lagres i en vektordatabase. Ved spørsmål hentes relevante chunks (similarity/MMR) og “stuffes” inn i promptkonteksten før LLM genererer et svar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "480c8374",
      "metadata": {},
      "source": [
        "## 9) Formler (LaTeX) + Python-ekvivalenter\n",
        "\n",
        "### Cosine similarity\n",
        "\n",
        "\\[\n",
        "\\cos(\\theta) = \\frac{\\vec a \\cdot \\vec b}{\\lVert \\vec a \\rVert \\, \\lVert \\vec b \\rVert}\n",
        "\\]\n",
        "\n",
        "### Dot product\n",
        "\n",
        "\\[\n",
        "\\vec a \\cdot \\vec b = \\sum_i a_i b_i\n",
        "\\]\n",
        "\n",
        "### Euclidean distance\n",
        "\n",
        "\\[\n",
        "\\lVert \\vec a - \\vec b \\rVert_2 = \\sqrt{\\sum_i (a_i - b_i)^2}\n",
        "\\]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c75ace3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Sequence\n",
        "\n",
        "\n",
        "def dot(a: Sequence[float], b: Sequence[float]) -> float:\n",
        "    if len(a) != len(b):\n",
        "        raise ValueError(\"Vectors must have same length\")\n",
        "    return sum(x * y for x, y in zip(a, b))\n",
        "\n",
        "\n",
        "def l2_norm(a: Sequence[float]) -> float:\n",
        "    return math.sqrt(dot(a, a))\n",
        "\n",
        "\n",
        "def cosine_similarity(a: Sequence[float], b: Sequence[float]) -> float:\n",
        "    denom = l2_norm(a) * l2_norm(b)\n",
        "    if denom == 0:\n",
        "        return 0.0\n",
        "    return dot(a, b) / denom\n",
        "\n",
        "\n",
        "def euclidean_distance(a: Sequence[float], b: Sequence[float]) -> float:\n",
        "    if len(a) != len(b):\n",
        "        raise ValueError(\"Vectors must have same length\")\n",
        "    return math.sqrt(sum((x - y) ** 2 for x, y in zip(a, b)))\n",
        "\n",
        "\n",
        "a = [1.0, 2.0, 3.0]\n",
        "b = [2.0, 0.0, 4.0]\n",
        "\n",
        "{\n",
        "    \"dot\": dot(a, b),\n",
        "    \"cosine\": cosine_similarity(a, b),\n",
        "    \"euclidean\": euclidean_distance(a, b),\n",
        "}\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
