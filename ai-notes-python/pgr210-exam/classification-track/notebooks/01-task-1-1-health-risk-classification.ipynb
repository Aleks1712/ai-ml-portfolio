{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1.1 – Klassifisering: health_risk (Low/Medium/High)\n",
        "\n",
        "## Mål (sensor-vennlig)\n",
        "Målet er å bygge og evaluere flere klassifiseringsmodeller for å predikere `health_risk` (Low/Medium/High). Fokus er **ikke bare kode**, men:\n",
        "\n",
        "- **begrunnede valg** (imputering/encoding/skalering/modellvalg)\n",
        "- **systematisk evaluering** (confusion matrix, accuracy, macro F1)\n",
        "- **feilanalyse** og konkrete forbedringsforslag\n",
        "\n",
        "## Ord-budsjett (tips)\n",
        "Hold teksten kort: problem → metode → resultater → feilanalyse → forbedringer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Imports + config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Load dataset\n",
        "\n",
        "Legg datasettet ditt som CSV her (eksempel):\n",
        "- `data/customer_health_profile.csv`\n",
        "\n",
        "Bytt `DATA_PATH` om filnavnet ditt er annerledes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: legg inn riktig filsti\n",
        "# Notebooken ligger i notebooks/, derfor bruker vi ../data/\n",
        "DATA_PATH = Path(\"../data/customer_health_profile.csv\")\n",
        "\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Fant ikke {DATA_PATH}. Lag en data/-mappe ved siden av notebooken og legg inn CSV-en der.\"\n",
        "    )\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Quick EDA (kort og relevant)\n",
        "\n",
        "- Datatyper + missing values\n",
        "- Class balance for `health_risk`\n",
        "- En kort kommentar: er datasettet balansert? hva betyr det for metrics?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_col = \"health_risk\"  # TODO: bytt hvis target heter noe annet\n",
        "\n",
        "print(\"shape:\", df.shape)\n",
        "display(df.dtypes.value_counts())\n",
        "\n",
        "missing = df.isna().mean().sort_values(ascending=False)\n",
        "display(missing.head(20))\n",
        "\n",
        "display(df[target_col].value_counts(dropna=False))\n",
        "display(df[target_col].value_counts(normalize=True, dropna=False).rename(\"proportion\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Split (train/test) + leakage-sjekk\n",
        "\n",
        "Vi bruker **stratified split** siden dette er en klassifisering. Hold test-settet “rent” (ikke bruk det under tuning).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y,\n",
        ")\n",
        "\n",
        "print(\"train:\", X_train.shape, \"test:\", X_test.shape)\n",
        "display(y_train.value_counts(normalize=True).rename(\"train_prop\"))\n",
        "display(y_test.value_counts(normalize=True).rename(\"test_prop\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Preprocessing pipeline (imputering + encoding + scaling)\n",
        "\n",
        "Sensor-fokus (skriv 5–8 setninger):\n",
        "- **Imputering**: numerisk median, kategorisk mest-frekvent (robust + enkel baseline)\n",
        "- **Encoding**: OneHot for kategoriske variabler\n",
        "- **Scaling**: StandardScaler for modeller som er sensitive (SVM/KNN/LogReg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
        "cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
        "\n",
        "numeric_pipe = Pipeline(\n",
        "    steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "    ]\n",
        ")\n",
        "\n",
        "categorical_pipe = Pipeline(\n",
        "    steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
        "    ]\n",
        ")\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_pipe, num_cols),\n",
        "        (\"cat\", categorical_pipe, cat_cols),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"num_cols:\", len(num_cols), \"cat_cols:\", len(cat_cols))\n",
        "num_cols[:10], cat_cols[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Tren og sammenlign flere modeller (baseline → best)\n",
        "\n",
        "Vi sammenligner modeller med cross-validation (stratified). Rapportér **accuracy** og **macro F1**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models: Dict[str, object] = {\n",
        "    \"logreg\": LogisticRegression(max_iter=2000),\n",
        "    \"svm_rbf\": SVC(kernel=\"rbf\"),\n",
        "    \"knn\": KNeighborsClassifier(),\n",
        "    # NB: GaussianNB kan ikke direkte ta sparse output fra OneHotEncoder.\n",
        "    # Hvis du vil bruke NB, må vi enten gjøre onehot dense eller bruke BernoulliNB/MultinomialNB på passende features.\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "rows = []\n",
        "for name, clf in models.items():\n",
        "    pipe = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", clf)])\n",
        "    scores = cross_validate(\n",
        "        pipe,\n",
        "        X_train,\n",
        "        y_train,\n",
        "        cv=cv,\n",
        "        scoring={\"acc\": \"accuracy\", \"f1_macro\": \"f1_macro\"},\n",
        "        n_jobs=None,\n",
        "        return_train_score=False,\n",
        "    )\n",
        "    rows.append(\n",
        "        {\n",
        "            \"model\": name,\n",
        "            \"acc_mean\": float(np.mean(scores[\"test_acc\"])),\n",
        "            \"acc_std\": float(np.std(scores[\"test_acc\"])),\n",
        "            \"f1_macro_mean\": float(np.mean(scores[\"test_f1_macro\"])),\n",
        "            \"f1_macro_std\": float(np.std(scores[\"test_f1_macro\"])),\n",
        "        }\n",
        "    )\n",
        "\n",
        "results = pd.DataFrame(rows).sort_values([\"f1_macro_mean\", \"acc_mean\"], ascending=False)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Fit beste modell på train → eval på test\n",
        "\n",
        "Her viser du:\n",
        "- confusion matrix\n",
        "- accuracy + macro F1\n",
        "- kort feilanalyse (hvilke klasser blandes?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_name = results.iloc[0][\"model\"]\n",
        "best_clf = models[str(best_name)]\n",
        "best_pipe = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", best_clf)])\n",
        "\n",
        "best_pipe.fit(X_train, y_train)\n",
        "pred = best_pipe.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, pred)\n",
        "f1m = f1_score(y_test, pred, average=\"macro\")\n",
        "print(\"best:\", best_name)\n",
        "print(\"accuracy:\", acc)\n",
        "print(\"macro_f1:\", f1m)\n",
        "\n",
        "print(\"\\nclassification_report:\\n\", classification_report(y_test, pred))\n",
        "confusion_matrix(y_test, pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Feilanalyse (konkret og kort)\n",
        "\n",
        "Finn noen eksempler på feilklassifiseringer (f.eks. High→Medium) og beskriv mønsteret.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "errors = X_test.copy()\n",
        "errors[\"y_true\"] = y_test.values\n",
        "errors[\"y_pred\"] = pred\n",
        "\n",
        "wrong = errors[errors[\"y_true\"] != errors[\"y_pred\"]]\n",
        "print(\"wrong:\", len(wrong), \"/\", len(errors))\n",
        "wrong.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Refleksjon / forbedringer (sensor-points)\n",
        "\n",
        "Skriv 8–12 linjer om:\n",
        "- ubalanse i klasser → hvorfor macro F1 er relevant\n",
        "- bedre imputering (IterativeImputer/KNNImputer) vs enkel baseline\n",
        "- hyperparameter tuning (GridSearchCV) + hvorfor\n",
        "- feature importance / forklarbarhet (permutation importance)\n",
        "- begrensninger: datasettstørrelse, label-støy, generaliserbarhet\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
