{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "749d905e",
      "metadata": {},
      "source": [
        "# Rapport-notater: BERT (Extractive QA) + XLNet (klassifisering)\n",
        "\n",
        "Strukturerte notater du kan bruke direkte i bachelor-/rapporttekst.\n",
        "\n",
        "## Innhold\n",
        "\n",
        "- Transformer: encoder vs decoder\n",
        "- BERT QA: inputformat, start/end logits, softmax og span-valg\n",
        "- XLNet klassifisering: preprocessing, label encoding, padding/attention_mask\n",
        "- Lagring + inferens\n",
        "- Kritisk refleksjon (metrikker, datasettstørrelse, truncation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3130dbe",
      "metadata": {},
      "source": [
        "## 1) Transformer: encoder vs decoder\n",
        "\n",
        "- **Encoder-only** (BERT/RoBERTa/DistilBERT): forståelse/representasjon\n",
        "- **Decoder-only** (GPT): generering\n",
        "- **Encoder–decoder** (T5/BART): seq2seq (oversettelse/summarization)\n",
        "\n",
        "Huske-regel:\n",
        "\n",
        "- Encoder = les/forstå\n",
        "- Decoder = skriv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2e6ad35",
      "metadata": {},
      "source": [
        "## 2) Pipeline A: BERT Question Answering (Extractive QA)\n",
        "\n",
        "### 2.1 Inputformat\n",
        "\n",
        "Vanlig format:\n",
        "\n",
        "- `[CLS] question [SEP] context [SEP]`\n",
        "\n",
        "Segmentering (`token_type_ids`) markerer ofte:\n",
        "\n",
        "- 0 = spørsmål\n",
        "- 1 = kontekst\n",
        "\n",
        "### 2.2 Modelloutput\n",
        "\n",
        "`BertForQuestionAnswering` gir:\n",
        "\n",
        "- `start_logits`: score per token for start\n",
        "- `end_logits`: score per token for slutt\n",
        "\n",
        "### 2.3 Softmax (logits → sannsynlighet)\n",
        "\n",
        "\\[\n",
        "\\mathrm{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
        "\\]\n",
        "\n",
        "I praksis kan sannsynlighet brukes som confidence-score.\n",
        "\n",
        "### 2.4 Span-valg\n",
        "\n",
        "En robust strategi velger (i, j) der j ≥ i og ofte med maks-lengde.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf204214",
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT QA (kode-mal) — krever transformers + torch\n",
        "\n",
        "# from transformers import BertForQuestionAnswering, BertTokenizer\n",
        "# import torch\n",
        "#\n",
        "# model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "# model = BertForQuestionAnswering.from_pretrained(model_name)\n",
        "# tok = BertTokenizer.from_pretrained(model_name)\n",
        "#\n",
        "# question = \"When was the first DVD released?\"\n",
        "# context = \"The first DVD (Digital Versatile Disc) was released on March 24, 1997...\"\n",
        "#\n",
        "# enc = tok.encode_plus(question, context, return_tensors=\"pt\")\n",
        "# with torch.no_grad():\n",
        "#     out = model(**enc)\n",
        "#\n",
        "# start_idx = out.start_logits.argmax(dim=-1).item()\n",
        "# end_idx = out.end_logits.argmax(dim=-1).item()\n",
        "#\n",
        "# input_ids = enc[\"input_ids\"][0]\n",
        "# tokens = tok.convert_ids_to_tokens(input_ids)\n",
        "# answer = tok.convert_tokens_to_string(tokens[start_idx : end_idx + 1])\n",
        "# print(answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f83623d",
      "metadata": {},
      "source": [
        "## 3) Pipeline B: XLNet tekstklassifisering (emosjoner)\n",
        "\n",
        "### 3.1 Supervised learning\n",
        "\n",
        "Mål: lære mappingen **tekst → klasse**.\n",
        "\n",
        "### 3.2 Preprocessing\n",
        "\n",
        "Rensing (fjerne støy som emoji/mentions) kan stabilisere trening.\n",
        "\n",
        "### 3.3 Klassebalanse\n",
        "\n",
        "Hvis datasettet er ubalansert, kan modellen lære å alltid gjette majoritetsklassen.\n",
        "\n",
        "### 3.4 Tokenisering og embeddings (presis formulering)\n",
        "\n",
        "Riktig beskrivelse:\n",
        "\n",
        "- Tokenizer lager `input_ids` + `attention_mask`\n",
        "- Embeddings genereres inne i modellens embedding-lag når `input_ids` mates inn\n",
        "\n",
        "### 3.5 Padding/truncation\n",
        "\n",
        "- `padding=\"max_length\"` + `max_length=128` gir mange `<pad>`\n",
        "- `attention_mask` styrer hva modellen ignorerer\n",
        "\n",
        "### 3.6 Resultat (tolkning)\n",
        "\n",
        "Lav accuracy på veldig lite treningsdata kan bety at pipeline er korrekt, men datasettet er for lite.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bbbb27f",
      "metadata": {},
      "source": [
        "## 4) Kritisk refleksjon (gir pluss i vurdering)\n",
        "\n",
        "- **Datasettstørrelse**: 100 samples ≈ tilfeldig nivå for 4 klasser (~0.25)\n",
        "- **Metrikker**: rapporter også F1 per klasse + confusion matrix\n",
        "- **QA span**: `argmax` på start/end kan gi suboptimale spans → bruk best-span søk med j ≥ i\n",
        "- **Tokenisering/padding**: dokumenter strategi og verifiser `attention_mask`\n",
        "- **Truncation**: evaluer ASR/utility med og uten truncation (sikkerhetsrelevant)\n",
        "\n",
        "### Rapportformulering (klar og kort)\n",
        "\n",
        "> “We saved intermediate checkpoints and evaluated both clean utility and attack success under different tokenization and truncation settings to ensure reproducibility and robust security evaluation.”\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f110d76",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Softmax i Python (samme som formelen i LaTeX)\n",
        "import math\n",
        "from typing import Sequence, List\n",
        "\n",
        "\n",
        "def softmax(logits: Sequence[float]) -> List[float]:\n",
        "    # Stabil softmax: trekk fra max-logit for å unngå overflow\n",
        "    m = max(logits)\n",
        "    exps = [math.exp(z - m) for z in logits]\n",
        "    s = sum(exps)\n",
        "    return [e / s for e in exps]\n",
        "\n",
        "\n",
        "z = [1.2, 0.3, -2.0]\n",
        "softmax(z), sum(softmax(z))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
